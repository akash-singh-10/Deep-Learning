{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXNKyEUKNq3jLaWRRI2pHG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akash-singh-10/Deep-Learning/blob/main/DLA5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "neKVaex2hHXT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Lambda\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,|\n",
        "we conjure the spirits of the computer with our spells.\"\"\""
      ],
      "metadata": {
        "id": "ZH4bNnwph7MY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = data.split(\".\")\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqhW1RLqh998",
        "outputId": "fe84466f-6627-43cf-d68b-b5685ffee405"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['We are about to study the idea of a computational process',\n",
              " '\\nComputational processes are abstract beings that inhabit computers',\n",
              " '\\nAs they evolve, processes manipulate other abstract things called data',\n",
              " '\\nThe evolution of a process is directed by a pattern of rules\\ncalled a program',\n",
              " ' People create programs to direct processes',\n",
              " ' In effect,|\\nwe conjure the spirits of the computer with our spells',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean = []\n",
        "for sentence in sentences:\n",
        "  if sentence == \"\":\n",
        "    continue;\n",
        "  sentence = re.sub('[^A-Za-z0-9]+',' ', sentence)\n",
        "  sentence = re.sub(r'(?:^| )\\w(?:$| )',' ', sentence).strip()\n",
        "  sentence = sentence.lower()\n",
        "  clean.append(sentence)\n",
        "clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dt8darNKiM3Y",
        "outputId": "45189043-385b-4634-b935-71f61f77a0c8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['we are about to study the idea of computational process',\n",
              " 'computational processes are abstract beings that inhabit computers',\n",
              " 'as they evolve processes manipulate other abstract things called data',\n",
              " 'the evolution of process is directed by pattern of rules called program',\n",
              " 'people create programs to direct processes',\n",
              " 'in effect we conjure the spirits of the computer with our spells']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "collec = clean"
      ],
      "metadata": {
        "id": "MYbpTFlPi9xz"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(collec)\n",
        "seq = tokenizer.texts_to_sequences(collec)\n",
        "print(seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGDv8ytVjkel",
        "outputId": "4c49ecfd-85d5-4508-c938-4f4b021d3e5e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4, 5, 11, 6, 12, 1, 13, 2, 7, 8], [7, 3, 5, 9, 14, 15, 16, 17], [18, 19, 20, 3, 21, 22, 9, 23, 10, 24], [1, 25, 2, 8, 26, 27, 28, 29, 2, 30, 10, 31], [32, 33, 34, 6, 35, 3], [36, 37, 4, 38, 1, 39, 2, 1, 40, 41, 42, 43]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i2word = {}\n",
        "word2i = {}\n",
        "for ind1, sequence in enumerate(seq):\n",
        "  print(sequence)\n",
        "  words = clean[ind1].split()\n",
        "  print(words)\n",
        "  for ind2, val in enumerate(sequence):\n",
        "    i2word[val] = words[ind2]\n",
        "    word2i[words[ind2]] = val\n",
        "\n",
        "print(i2word)\n",
        "print(word2i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwXPoQq4kVaG",
        "outputId": "9446f2c1-873d-40d0-d32f-b6e1ed02a601"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 5, 11, 6, 12, 1, 13, 2, 7, 8]\n",
            "['we', 'are', 'about', 'to', 'study', 'the', 'idea', 'of', 'computational', 'process']\n",
            "[7, 3, 5, 9, 14, 15, 16, 17]\n",
            "['computational', 'processes', 'are', 'abstract', 'beings', 'that', 'inhabit', 'computers']\n",
            "[18, 19, 20, 3, 21, 22, 9, 23, 10, 24]\n",
            "['as', 'they', 'evolve', 'processes', 'manipulate', 'other', 'abstract', 'things', 'called', 'data']\n",
            "[1, 25, 2, 8, 26, 27, 28, 29, 2, 30, 10, 31]\n",
            "['the', 'evolution', 'of', 'process', 'is', 'directed', 'by', 'pattern', 'of', 'rules', 'called', 'program']\n",
            "[32, 33, 34, 6, 35, 3]\n",
            "['people', 'create', 'programs', 'to', 'direct', 'processes']\n",
            "[36, 37, 4, 38, 1, 39, 2, 1, 40, 41, 42, 43]\n",
            "['in', 'effect', 'we', 'conjure', 'the', 'spirits', 'of', 'the', 'computer', 'with', 'our', 'spells']\n",
            "{4: 'we', 5: 'are', 11: 'about', 6: 'to', 12: 'study', 1: 'the', 13: 'idea', 2: 'of', 7: 'computational', 8: 'process', 3: 'processes', 9: 'abstract', 14: 'beings', 15: 'that', 16: 'inhabit', 17: 'computers', 18: 'as', 19: 'they', 20: 'evolve', 21: 'manipulate', 22: 'other', 23: 'things', 10: 'called', 24: 'data', 25: 'evolution', 26: 'is', 27: 'directed', 28: 'by', 29: 'pattern', 30: 'rules', 31: 'program', 32: 'people', 33: 'create', 34: 'programs', 35: 'direct', 36: 'in', 37: 'effect', 38: 'conjure', 39: 'spirits', 40: 'computer', 41: 'with', 42: 'our', 43: 'spells'}\n",
            "{'we': 4, 'are': 5, 'about': 11, 'to': 6, 'study': 12, 'the': 1, 'idea': 13, 'of': 2, 'computational': 7, 'process': 8, 'processes': 3, 'abstract': 9, 'beings': 14, 'that': 15, 'inhabit': 16, 'computers': 17, 'as': 18, 'they': 19, 'evolve': 20, 'manipulate': 21, 'other': 22, 'things': 23, 'called': 10, 'data': 24, 'evolution': 25, 'is': 26, 'directed': 27, 'by': 28, 'pattern': 29, 'rules': 30, 'program': 31, 'people': 32, 'create': 33, 'programs': 34, 'direct': 35, 'in': 36, 'effect': 37, 'conjure': 38, 'spirits': 39, 'computer': 40, 'with': 41, 'our': 42, 'spells': 43}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index)+1\n",
        "embedding_size = 10\n",
        "window = 2\n",
        "\n",
        "contexts = []\n",
        "targets = []\n",
        "\n",
        "for s in seq:\n",
        "  for i in range(window, len(s)-window):\n",
        "    context = s[i-window:i] + s[i+1 : i+1+window]\n",
        "    target = s[i]\n",
        "    contexts.append(context)\n",
        "    targets.append(target)\n",
        "\n",
        "for i in range(5):\n",
        "  words=[]\n",
        "  target = i2word.get(targets[i])\n",
        "  for j in contexts[i]:\n",
        "    words.append(i2word.get(j))\n",
        "  print(words, \"->\", target)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2AmLRCvluCW",
        "outputId": "aa7a91f5-5add-485c-e1e0-5019b3076954"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['we', 'are', 'to', 'study'] -> about\n",
            "['are', 'about', 'study', 'the'] -> to\n",
            "['about', 'to', 'the', 'idea'] -> study\n",
            "['to', 'study', 'idea', 'of'] -> the\n",
            "['study', 'the', 'of', 'computational'] -> idea\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(contexts)\n",
        "Y = np.array(targets)"
      ],
      "metadata": {
        "id": "_7NNWqqznVYR"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim = vocab_size, output_dim=embedding_size, input_length=2*window))\n",
        "model.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(units=vocab_size, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVS1Eh9hnzTp",
        "outputId": "9a0ed434-2e3f-4be0-9ec8-c35e45802e88"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 4, 10)             440       \n",
            "                                                                 \n",
            " lambda_1 (Lambda)           (None, 10)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 256)               2816      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               131584    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 44)                22572     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 157412 (614.89 KB)\n",
            "Trainable params: 157412 (614.89 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X,Y,epochs=200, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QxmW3hMonoh",
        "outputId": "7a746d73-ce32-4d7d-9c78-5317bee62fbc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 1s 12ms/step - loss: 3.7845 - accuracy: 0.0294\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 3.7754 - accuracy: 0.1176\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 3.7686 - accuracy: 0.1176\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 3.7610 - accuracy: 0.1176\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 3.7516 - accuracy: 0.1176\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 3.7407 - accuracy: 0.1176\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 3.7260 - accuracy: 0.1176\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 3.7097 - accuracy: 0.1176\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 3.6886 - accuracy: 0.1176\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 3.6636 - accuracy: 0.1176\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 3.6342 - accuracy: 0.1176\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3.5993 - accuracy: 0.1176\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 3.5591 - accuracy: 0.1176\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 3.5151 - accuracy: 0.1176\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.4655 - accuracy: 0.1176\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.4204 - accuracy: 0.1176\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3.3764 - accuracy: 0.1176\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 3.3362 - accuracy: 0.1176\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 3.3007 - accuracy: 0.1176\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 3.2704 - accuracy: 0.1176\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.2428 - accuracy: 0.1176\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 3.2248 - accuracy: 0.1176\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 3.2112 - accuracy: 0.1176\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 3.1975 - accuracy: 0.1176\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.1740 - accuracy: 0.1176\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 3.1452 - accuracy: 0.1176\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 3.1153 - accuracy: 0.1176\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 3.0895 - accuracy: 0.1176\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 3.0610 - accuracy: 0.1176\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.0341 - accuracy: 0.1471\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.0088 - accuracy: 0.1765\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.9867 - accuracy: 0.1765\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.9635 - accuracy: 0.2353\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.9423 - accuracy: 0.2353\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.9189 - accuracy: 0.2647\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.8949 - accuracy: 0.2941\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.8630 - accuracy: 0.3235\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.8254 - accuracy: 0.3235\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.7848 - accuracy: 0.3235\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 2.7475 - accuracy: 0.2941\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 2.7119 - accuracy: 0.2941\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.6766 - accuracy: 0.2941\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.6393 - accuracy: 0.2941\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.6032 - accuracy: 0.2941\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.5651 - accuracy: 0.2941\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.5260 - accuracy: 0.2941\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.4905 - accuracy: 0.2941\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 2.4554 - accuracy: 0.3529\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.4148 - accuracy: 0.3529\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.3669 - accuracy: 0.3529\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 2.3268 - accuracy: 0.3529\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.2806 - accuracy: 0.3529\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.2438 - accuracy: 0.3529\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.2084 - accuracy: 0.3824\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.1649 - accuracy: 0.3824\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.1132 - accuracy: 0.3824\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.0645 - accuracy: 0.3824\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.0179 - accuracy: 0.3824\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.9855 - accuracy: 0.3529\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.9574 - accuracy: 0.4118\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.9335 - accuracy: 0.4118\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.9072 - accuracy: 0.4118\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.8754 - accuracy: 0.4706\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.8340 - accuracy: 0.4706\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.7993 - accuracy: 0.4706\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.7709 - accuracy: 0.5000\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.7457 - accuracy: 0.4412\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.7176 - accuracy: 0.4118\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.6918 - accuracy: 0.4118\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.6725 - accuracy: 0.4118\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.6595 - accuracy: 0.4118\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.6421 - accuracy: 0.3824\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.6178 - accuracy: 0.4706\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.5897 - accuracy: 0.4706\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.5548 - accuracy: 0.5000\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.5199 - accuracy: 0.5294\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.4833 - accuracy: 0.5588\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.4438 - accuracy: 0.5588\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.3997 - accuracy: 0.5588\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.3697 - accuracy: 0.5882\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.3526 - accuracy: 0.5294\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.3338 - accuracy: 0.5000\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.3153 - accuracy: 0.5000\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.2925 - accuracy: 0.5294\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.2587 - accuracy: 0.5588\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.2324 - accuracy: 0.5588\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.2102 - accuracy: 0.5882\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.1996 - accuracy: 0.6176\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.1813 - accuracy: 0.6471\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.1694 - accuracy: 0.5882\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.1418 - accuracy: 0.5882\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.1082 - accuracy: 0.5882\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.0829 - accuracy: 0.6176\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.0545 - accuracy: 0.6471\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.0296 - accuracy: 0.6471\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.0121 - accuracy: 0.6765\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.9937 - accuracy: 0.6471\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.9743 - accuracy: 0.6471\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.9630 - accuracy: 0.7059\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.9441 - accuracy: 0.7353\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.9247 - accuracy: 0.7353\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.8990 - accuracy: 0.7353\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.8707 - accuracy: 0.7647\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.8471 - accuracy: 0.7059\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.8323 - accuracy: 0.6765\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.8065 - accuracy: 0.6765\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.7747 - accuracy: 0.6765\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.7497 - accuracy: 0.7941\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.7279 - accuracy: 0.7941\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7118 - accuracy: 0.7941\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.7015 - accuracy: 0.8529\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.7045 - accuracy: 0.8235\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.7023 - accuracy: 0.8824\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6859 - accuracy: 0.8824\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6471 - accuracy: 0.8824\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.6116 - accuracy: 0.8824\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5880 - accuracy: 0.9412\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5784 - accuracy: 0.9118\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5762 - accuracy: 0.9118\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.5654 - accuracy: 0.9118\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.5589 - accuracy: 0.9412\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.5577 - accuracy: 0.8824\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.5477 - accuracy: 0.9118\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.5374 - accuracy: 0.9118\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.5235 - accuracy: 0.9118\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5166 - accuracy: 0.8824\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.5131 - accuracy: 0.8529\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.5109 - accuracy: 0.8824\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5014 - accuracy: 0.8824\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4950 - accuracy: 0.9118\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4887 - accuracy: 0.9118\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.4789 - accuracy: 0.8824\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.4574 - accuracy: 0.8824\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4381 - accuracy: 0.9118\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.4267 - accuracy: 0.9118\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4157 - accuracy: 0.9118\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3953 - accuracy: 0.9118\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3709 - accuracy: 0.9412\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3475 - accuracy: 0.9706\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3360 - accuracy: 0.9706\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3257 - accuracy: 0.9706\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3181 - accuracy: 0.9412\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3120 - accuracy: 0.9412\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3051 - accuracy: 0.9412\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2988 - accuracy: 0.9412\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2931 - accuracy: 0.9412\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2866 - accuracy: 0.9412\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2797 - accuracy: 0.9412\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2726 - accuracy: 0.9412\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2645 - accuracy: 0.9706\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2571 - accuracy: 0.9706\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.2501 - accuracy: 0.9706\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2416 - accuracy: 0.9706\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2344 - accuracy: 0.9706\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2285 - accuracy: 0.9706\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2224 - accuracy: 0.9706\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2187 - accuracy: 0.9706\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2151 - accuracy: 0.9706\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2107 - accuracy: 0.9706\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2064 - accuracy: 0.9706\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2009 - accuracy: 0.9706\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1948 - accuracy: 0.9706\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1893 - accuracy: 0.9706\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1855 - accuracy: 0.9706\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1748 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1702 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1653 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1629 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1626 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1629 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1560 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1494 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1469 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1463 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1450 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1426 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1395 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1368 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1347 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1323 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1299 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1263 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1200 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1141 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1130 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1142 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1166 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1191 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1212 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1240 - accuracy: 0.9706\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1249 - accuracy: 0.9706\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1239 - accuracy: 0.9706\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1216 - accuracy: 0.9706\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1172 - accuracy: 0.9706\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1120 - accuracy: 0.9706\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1058 - accuracy: 0.9706\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1000 - accuracy: 0.9706\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0948 - accuracy: 0.9706\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0910 - accuracy: 0.9706\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0887 - accuracy: 0.9706\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b510aaf36a0>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = [\n",
        "    \"we are to study\",\n",
        "    \"create programs direct processes\",\n",
        "    \"spirits process study program\",\n",
        "    \"idea study people create\"\n",
        "]"
      ],
      "metadata": {
        "id": "3U2zAvazpW9p"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in test:\n",
        "  words = t.split(\" \")\n",
        "  x_test = []\n",
        "  for i in words:\n",
        "    x_test.append(word2i.get(i))\n",
        "  x_test = np.array([x_test])\n",
        "  print(words)\n",
        "  pred = model.predict(x_test)\n",
        "  print(i2word.get(np.argmax(pred[0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bz5i2WwhqFKO",
        "outputId": "717e8ce8-e090-4dfd-c54a-26bcb82978b6"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['we', 'are', 'to', 'study']\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "about\n",
            "['create', 'programs', 'direct', 'processes']\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "to\n",
            "['spirits', 'process', 'study', 'program']\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "the\n",
            "['idea', 'study', 'people', 'create']\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "programs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Wjzev00q8uR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}